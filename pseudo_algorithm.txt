
total_stpes = 0
initialize replay_buffer, agent

WHILE total_episodes < training_episodes:
	# INTERACTION START #######################################
	end_of_episode = false
	actual_trayectory = []
	get a workspace (Either sample a new or just take one out from some pre-generated workspaces)
	
	set up an environment with that workspace
	state = environment.state
	
	WHILE not end_of_episode:
		get action from actor_NN for the given state
		get next_state, reward and end_of_episode flag from environment after taking that action
		push (state, action, reward, next_state) to actual_trajectory
		state = next_state
	END WHILE
	
	IF actual_trajectory was successful:
		save every (state, action, reward, next_state) from actual_trajectory to replay_buffer
	ELSE:
		save every (state, action, reward, next_state) from actual_trajectory to replay_buffer
		get new successful workspace for actual_trajectory
		modify the states and next_states in the replay buffer so that they include the new workspace
		save every (state, action, reward, next_state) from actual_trajectory to replay_buffer with the new workspace
	END IF

	total_episodes = total_episodes + 1
	
	# INTERACTION END ##########################################

	# TRAINING START ###########################################

	sample a batch from the replay_buffer


	target_Qs = batch_rewards + target_critic_NN(batch_next_states, target_actor_NN(batch_next_states))
	predicted_Qs = critic_NN(batch_states, batch_actions)
	Q_network_loss = target_Qs - predicted_Qs
	train the critic_NN based on the Q_network_loss
	update the target_critic_NN with tau and with the critic_network


	reward = critic_NN(batch_states, actor_NN(batch_states))*actor_NN(batch_states)
	train actor_NN based on the reward
	update the target_actor_NN with tau and with actor_NN

	# TRAINING END #############################################

END WHILE

	
	
	
	



	
