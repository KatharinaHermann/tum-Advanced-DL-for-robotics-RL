diff --git a/gym-pointrobo/gym_pointrobo/envs/__pycache__/pointrobo_env.cpython-37.pyc b/gym-pointrobo/gym_pointrobo/envs/__pycache__/pointrobo_env.cpython-37.pyc
index 9164020..191d6e4 100644
Binary files a/gym-pointrobo/gym_pointrobo/envs/__pycache__/pointrobo_env.cpython-37.pyc and b/gym-pointrobo/gym_pointrobo/envs/__pycache__/pointrobo_env.cpython-37.pyc differ
diff --git a/gym-pointrobo/gym_pointrobo/envs/pointrobo_env.py b/gym-pointrobo/gym_pointrobo/envs/pointrobo_env.py
index 3ff5867..eb9e82e 100644
--- a/gym-pointrobo/gym_pointrobo/envs/pointrobo_env.py
+++ b/gym-pointrobo/gym_pointrobo/envs/pointrobo_env.py
@@ -42,7 +42,7 @@ class PointroboEnv(gym.Env):
         #The observation will be the coordinate of the agent 
         #this can be described by Box space
         self.observation_space = spaces.Box(low=0.0, high=self.grid_size,
-                                            shape=(2,), dtype=np.float32)
+                                            shape=(20,), dtype=np.float32)
 
         # Initialize the agent
         self.current_step = 0
@@ -143,7 +143,7 @@ class PointroboEnv(gym.Env):
 
         #print("Distance to the nearest obstacle at the center: ", nearest_dist)
             
-        if nearest_dist <= 1.5:
+        if nearest_dist <= 2:
             collision = True
             #print("Collision is: ", collision)
         
diff --git a/hwr/training/pointrobot_trainer.py b/hwr/training/pointrobot_trainer.py
index 1fe8fec..4f81586 100644
--- a/hwr/training/pointrobot_trainer.py
+++ b/hwr/training/pointrobot_trainer.py
@@ -110,6 +110,7 @@ class PointrobotTrainer:
         #Concatenate position observation with start, goal, and reduced workspace
         reduced_workspace = self._CAE.evaluate(workspace)
         obs_full = np.concatenate((obs, goal, reduced_workspace))
+        
 
         while total_steps < self._max_steps:
             #Get action randomly for warmup /from Actor-NN otherwise
@@ -353,8 +354,8 @@ class PointrobotTrainer:
                             help='Interval to save model')
         parser.add_argument('--save-summary-interval', type=int, default=int(1e3),
                             help='Interval to save summary')
-        parser.add_argument('--model-dir', type=str, default='../models/agents',
-                            help='Directory to restore model. default =  ../models/agents')
+        parser.add_argument('--model-dir', type=str, default='models/agents',
+                            help='Directory to restore model. default =  models/agents')
         parser.add_argument('--dir-suffix', type=str, default='',
                             help='Suffix for directory that contains results')
         parser.add_argument('--normalize-obs', action='store_true',
@@ -393,7 +394,8 @@ class PointrobotTrainer:
                             help='latent dimension of the CAE. default: 16')
         parser.add_argument('--cae_conv_filters', type=int, nargs='+', default=[4, 8, 16],
                             help='number of filters in the conv layers. default: [4, 8, 16]')
-        parser.add_argument('--cae_weights_path', type=str, default='../models/cae/model_num_5_size_8.h5',
-                            help='path to saved CAE weights. default: ../models/cae/model_num_5_size_8.h5')
+        parser.add_argument('--cae_weights_path', type=str, default='models/cae/model_num_5_size_8.h5',
+                            help='path to saved CAE weights. default: models/cae/model_num_5_size_8.h5')
+
+        return parser
 
-        return parser
\ No newline at end of file
diff --git a/tests/test_pointrobot_trainer.py b/tests/test_pointrobot_trainer.py
index bc4e4d8..91886a9 100644
--- a/tests/test_pointrobot_trainer.py
+++ b/tests/test_pointrobot_trainer.py
@@ -43,7 +43,7 @@ def test_state_concatenation():
                 input_shape=(32, 32),
                 conv_filters=[4, 8, 16])
     model.build(input_shape=(1, 32, 32, 1))
-    model.load_weights(filepath='../models/cae/model_num_5_size_8.h5')
+    model.load_weights(filepath='models/cae/model_num_5_size_8.h5')
 
     for layer, _ in model._get_trainable_state().items():
         layer.trainable = False
@@ -63,6 +63,8 @@ def test_evaluation():
     """Possibly a good ide to first test the eval_policy method of the agent.
     If that works, than debugging training may be easier.
     """
+    print('-' * 5 + 'test_evaluation' + '-' * 5)
+    
     total_steps = 10
 
     parser = PointrobotTrainer.get_argument()
@@ -97,13 +99,19 @@ def test_evaluation():
 
 
 def test_training():
+
+    print('-' * 5 + 'test_training' + '-' * 5)
+
     parser = PointrobotTrainer.get_argument()
     parser = DDPG.get_argument(parser)
     parser.add_argument('--env-name', type=str, default="pointrobo-v0")
     parser.set_defaults(batch_size=100)
-    parser.set_defaults(n_warmup=10000)
+    parser.set_defaults(n_warmup=10)
     args = parser.parse_args()
 
+    args.max_steps = 100
+    args.show_progress = True
+
     #######
     # possibly set some args attributes to small numbers, so that testing does not last that long.
     # like for example args.n_warmup = 10, args.max_steps = 100...
@@ -129,10 +137,9 @@ if __name__ == '__main__':
 
     test_pointrobot_trainer_init()
     test_state_concatenation()
-    #test_evaluation()
-    #test_training()
+    test_evaluation()
+    test_training()
     print('All tests have run successfully!')
-
     
 
     
