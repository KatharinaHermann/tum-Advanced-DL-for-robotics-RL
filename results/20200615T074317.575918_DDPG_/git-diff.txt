diff --git a/gym-pointrobo/gym_pointrobo/envs/__pycache__/pointrobo_env.cpython-37.pyc b/gym-pointrobo/gym_pointrobo/envs/__pycache__/pointrobo_env.cpython-37.pyc
index ff0cd16..bc7c153 100644
Binary files a/gym-pointrobo/gym_pointrobo/envs/__pycache__/pointrobo_env.cpython-37.pyc and b/gym-pointrobo/gym_pointrobo/envs/__pycache__/pointrobo_env.cpython-37.pyc differ
diff --git a/gym-pointrobo/gym_pointrobo/envs/pointrobo_env.py b/gym-pointrobo/gym_pointrobo/envs/pointrobo_env.py
index 3fd4c03..8991403 100644
--- a/gym-pointrobo/gym_pointrobo/envs/pointrobo_env.py
+++ b/gym-pointrobo/gym_pointrobo/envs/pointrobo_env.py
@@ -59,7 +59,7 @@ class PointroboEnv(gym.Env):
         self.current_step += 1        
 
         #Goal reached: Reward=1; Obstacle Hit: Reward=-1; Step made: Reward=-0.01
-        if (numpy.linalg.norm(self.agent_pos-self.goal_pos)<2).all(): 
+        if (np.linalg.norm(self.agent_pos-self.goal_pos)<2): 
             reward = 1
             done = True
         #Have we hit an obstacle?
diff --git a/hwr/training/pointrobot_trainer.py b/hwr/training/pointrobot_trainer.py
index 4f81586..78a876d 100644
--- a/hwr/training/pointrobot_trainer.py
+++ b/hwr/training/pointrobot_trainer.py
@@ -117,8 +117,14 @@ class PointrobotTrainer:
             
             if total_steps < self._policy.n_warmup:
                 action = self._env.action_space.sample()
+                action_norm = np.linalg.norm(action)
+                if action_norm != 0: 
+                    action = action / action_norm
             else:
                 action = self._policy.get_action(obs_full)
+                action_norm = np.linalg.norm(action)
+                if action_norm != 0: 
+                    action = action / action_norm
 
             #Take action and get next_obs, reward and done_flag from environment
             next_obs, reward, done, _ = self._env.step(action)
@@ -170,8 +176,8 @@ class PointrobotTrainer:
 
                 n_episode += 1
                 fps = episode_steps / (time.perf_counter() - episode_start_time)
-                self.logger.info("Total Epi: {0: 5} Steps: {1: 7} Episode Steps: {2: 5} Return: {3: 5.4f} FPS: {4:5.2f}".format(
-                    n_episode, total_steps, episode_steps, episode_return, fps))
+                self.logger.info("Total Epi: {0: 5} Steps: {1: 7} Episode Steps: {2: 5} Return: {3: 5.4f} Last reward: {4: 5.4f} FPS: {5: 5.2f}".format(
+                    n_episode, total_steps, episode_steps, episode_return, reward, fps))
                 tf.summary.scalar(
                     name="Common/training_return", data=episode_return)
 
diff --git a/src/train_pointrobot.py b/src/train_pointrobot.py
index 159a632..5c6e1f0 100644
--- a/src/train_pointrobot.py
+++ b/src/train_pointrobot.py
@@ -23,6 +23,7 @@ args = parser.parse_args()
 
 args.max_steps = 1e6
 args.test_interval = 50
+args.episode_max_steps = 100
 
 #Initialize the environment
 env = gym.make(args.env_name)
